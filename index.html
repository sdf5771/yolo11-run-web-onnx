<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>YOLO11 Run Web ONNX</title>
    <!-- ONNX Runtime CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.16.3/dist/ort.min.js"></script>
    <script>
        // WASM 파일 경로 설정
        const ort = window['ort'];
        const ortWasmBackendPath = 'https://cdn.jsdelivr.net/npm/onnxruntime-web@1.16.3/dist/';
        ort.env.wasm.wasmPaths = ortWasmBackendPath;
    </script>
    <!-- TensorFlow.js CDN -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.3.1/dist/tf.min.js"></script>
</head>
<body>
    <main style="display: flex; flex-direction: column; align-items: center;">
        <h1>YOLO11 Run Web ONNX</h1>
        <input type="file" id="fileInput" accept="image/*">
        <h3 id="detectResult">Detect Result: </h3>
        <div id="detectResultBox" style="display: flex; flex-direction: column; margin-top: 10px; border: 1px solid #000;"></div>
        <canvas id="canvas" style="margin-top: 10px; border: 1px solid #000;"></canvas>
    </main>  
    
    <script type="text/javascript">
        const fileInput = document.querySelector('#fileInput');
        const canvas = document.querySelector('#canvas');
        const detectResult = document.querySelector('#detectResult');
        const detectResultBox = document.querySelector('#detectResultBox');
        let modelSession = null;
        let ctx = null;

        // YOLO11n 모델이 지원하는 COCO 클래스 데이터셋
        const cocoClassesDataSet = [
            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',
            'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',
            'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',
            'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
            'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
            'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
            'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',
            'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',
            'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book',
            'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
        ];

        const getImageTensorFromPath = async (path) => {
            return new Promise((resolve, reject) => {
                try {
                    const imageElement = new Image();
                    imageElement.src = path;
                    // 이미지 로드 완료 후 텐서로 변환
                    imageElement.onload = () => {
                        const tensor = tf.browser.fromPixels(imageElement);
                        resolve(tensor);
                    }
                } catch (error) {
                    reject(error);
                }
            });
        }

        const preProcessImage = async (path) => {
            return new Promise((resolve, reject) => {
                if (!ctx) {
                    ctx = canvas.getContext('2d');
                }
                const imageElement = new Image();
                
                imageElement.onload = () => {
                    // 캔버스 크기를 원본 이미지 크기로 설정 (UI 표시용)
                    canvas.width = imageElement.width;
                    canvas.height = imageElement.height;
                    
                    // 원본 이미지 그리기
                    ctx.drawImage(imageElement, 0, 0, imageElement.width, imageElement.height);
        
                    // 모델 입력용 640x640 크기의 캔버스 생성
                    const inputCanvas = document.createElement('canvas');
                    inputCanvas.width = 640;
                    inputCanvas.height = 640;
                    const inputCtx = inputCanvas.getContext('2d');
                    
                    // 이미지 비율 유지하면서 640x640에 맞게 그리기
                    const scale = Math.min(640 / imageElement.width, 640 / imageElement.height);
                    const scaledWidth = imageElement.width * scale;
                    const scaledHeight = imageElement.height * scale;
                    const offsetX = (640 - scaledWidth) / 2;
                    const offsetY = (640 - scaledHeight) / 2;
                    
                    inputCtx.drawImage(imageElement, offsetX, offsetY, scaledWidth, scaledHeight);
                    
                    // 모델 입력용 이미지 데이터 가져오기
                    const ctxImageData = inputCtx.getImageData(0, 0, 640, 640);
                    const red = [], green = [], blue = [];
                    for (let i = 0; i < ctxImageData.data.length; i += 4) {
                        red.push(ctxImageData.data[i] / 255.0);
                        green.push(ctxImageData.data[i + 1] / 255.0);
                        blue.push(ctxImageData.data[i + 2] / 255.0);
                    }

                    // RGB 채널 순서로 변환
                    const imageInput = [...red, ...green, ...blue];
                    resolve([imageInput, 640, 640]); // 고정된 크기 반환
                };
                
                imageElement.onerror = () => {
                    reject(new Error('이미지 로드 실패'));
                };
                
                imageElement.src = path;
            });
        }

        /**
         * 사용 중인 브라우저의 WebGPU 지원 여부에 따라 WebGPU 또는 WASM 모델을 로드
         * @returns {Promise<ort.InferenceSession>}
        */
        const loadOptimizedModel = async () => {
            try {
                // 이미 모델이 로드되어 있으면 재사용
                if (modelSession) {
                    console.log('이미 로드된 모델 세션이 있으므로 기존 모델 세션을 재사용합니다.');
                    return modelSession;
                }
                // 더 자세한 디버깅 정보
                ort.env.debug = true;
                ort.env.logLevel = 'verbose';

                // 모델 경로
                const modelPath = "yolo11n.onnx";

                // 모델 옵션
                const options = {
                    graphOptimizationLevel: 'all',
                    enableCpuMemArena: true,
                    executionMode: 'sequential',
                    enableOrtCustomOps: true
                };

                // fetch를 사용한 모델 로드 방식
                const response = await fetch(modelPath);
                if (!response.ok) {
                    throw new Error(`모델 로드 실패: ${response.status} ${response.statusText}`);
                }
                
                const modelBuffer = await response.arrayBuffer();
                console.log('모델 파일 로드 완료, 크기:', modelBuffer.byteLength);

                // WebGPU 지원 여부 확인
                if(navigator.gpu) {
                    try {
                        // WebGPU 모델 로드
                        options.executionProviders = ['webgpu'];
                        console.log('WebGPU 모델 로드 시작');
                        modelSession = await ort.InferenceSession.create(modelPath, options);
                        return modelSession;
                    } catch (error) {
                        console.error('WebGPU 지원 없음:', error);
                    }   
                }
                
                // WASM 모델 로드
                options.executionProviders = ['wasm'];
                console.log('WASM 모델 로드 시작');
                modelSession = await ort.InferenceSession.create(modelPath, options);
                return modelSession;
            } catch (error) {
                console.error('모델 로드 오류:', error);
                throw error;
            }
        }

        const runModel = async (imageInput, width, height) => {
            try {
                const model = await loadOptimizedModel();
                console.log('모델 세션 생성 완료');
                
                // 모델의 입력 및 출력 이름 확인
                const inputNames = model.inputNames;
                const outputNames = model.outputNames;
                console.log('모델 입력 이름:', inputNames);
                console.log('모델 출력 이름:', outputNames);
                
                // 입력 이름이 배열에서 첫 번째 항목을 사용
                const inputName = inputNames[0];
                console.log('사용할 입력 이름:', inputName);

                const inputTensor = new ort.Tensor('float32', Float32Array.from(imageInput), [1, 3, width, height]);
                console.log('모델 입력 텐서 생성 완료, 형태:', [1, 3, width, height]);
                
                // 동적으로 입력 이름 사용
                const feeds = {};
                feeds[inputName] = inputTensor;
                
                const outputTensor = await model.run(feeds);
                console.log('outputTensor ', outputTensor);
                return outputTensor;
            } catch (error) {
                console.error('모델 실행 오류:', error);
                throw error;
            }
        }

        // IOU 계산 함수
        function calculateIOU(box1, box2) {
            const x1 = Math.max(box1.x, box2.x);
            const y1 = Math.max(box1.y, box2.y);
            const x2 = Math.min(box1.x + box1.width, box2.x + box2.width);
            const y2 = Math.min(box1.y + box1.height, box2.y + box2.height);
            
            if (x2 < x1 || y2 < y1) return 0;
            
            const intersection = (x2 - x1) * (y2 - y1);
            const area1 = box1.width * box1.height;
            const area2 = box2.width * box2.height;
            
            return intersection / (area1 + area2 - intersection);
        }
        
        // NMS 함수
        function applyNMS(detections, iouThreshold = 0.45) {
            // 신뢰도 기준 내림차순 정렬
            detections.sort((a, b) => b.confidence - a.confidence);
            
            const selectedDetections = [];
            const rejected = new Set();
            
            for (let i = 0; i < detections.length; i++) {
                if (rejected.has(i)) continue;
                
                selectedDetections.push(detections[i]);
                
                for (let j = i + 1; j < detections.length; j++) {
                    if (rejected.has(j)) continue;
                    
                    // 같은 클래스의 객체에 대해서만 NMS 적용
                    if (detections[i].classIndex === detections[j].classIndex) {
                        const iou = calculateIOU(detections[i].box, detections[j].box);
                        if (iou > iouThreshold) {
                            rejected.add(j);
                        }
                    }
                }
            }
            
            return selectedDetections;
        }

        const postProcessImage = async (outputTensor) => {
            // 출력 텐서의 구조 확인
            console.log('출력 텐서 구조:', outputTensor);
            
            // output0 키를 사용하여 실제 텐서 데이터에 접근
            const output = outputTensor['output0'];
            console.log('출력 텐서 데이터:', output);
            
            if (!output || !output.data) {
                console.error('출력 텐서 데이터가 없습니다.');
                return null;
            }
            
            // 출력 데이터의 형태와 크기 확인
            console.log('출력 데이터 형태:', output.dims);
            console.log('출력 데이터 크기:', output.size);
            
            // YOLO11n의 출력 형식은 [1, 84, 8400]
            // 84 = 4개 좌표 + 80개 클래스 확률
            const data = output.data;
            const numClasses = cocoClassesDataSet.length; // COCO 데이터셋의 클래스 수
            const numCoords = 4;  // x, y, width, height
            const numDetections = output.dims[2]; // 8400
            
            // 감지 정보를 담을 배열
            const detections = [];
            
            // 최소 신뢰도 임계값 설정
            const confidenceThreshold = 0.25;
            
            // 모든 감지 후보 처리
            for (let i = 0; i < numDetections; i++) {
                // 모든 클래스에 대한 최대 신뢰도 찾기
                let maxClassScore = 0;
                let maxClassIndex = -1;
                
                for (let c = 0; c < numClasses; c++) {
                    // 클래스 점수는 4번째 위치부터 시작
                    const classIdx = numCoords + c;
                    const classScore = data[classIdx * numDetections + i];
                    if (classScore > maxClassScore) {
                        maxClassScore = classScore;
                        maxClassIndex = c;
                    }
                }
                
                // 임계값보다 낮은 신뢰도는 무시
                if (maxClassScore < confidenceThreshold) continue;
                
                // 좌표 정보 추출 - 전치된 형식에서는 좌표가 다른 위치에 있음
                const x = data[0 * numDetections + i]; // x 좌표
                const y = data[1 * numDetections + i]; // y 좌표
                const width = data[2 * numDetections + i]; // 너비
                const height = data[3 * numDetections + i]; // 높이
                
                // 결과 추가
                detections.push({
                    box: { x, y, width, height },
                    confidence: maxClassScore,
                    classIndex: maxClassIndex,
                    className: cocoClassesDataSet[maxClassIndex]
                });
            }
            
            // 가장 높은 신뢰도를 가진 객체 하나만 반환 (테스트용)
            if (detections.length > 0) {
                // 신뢰도 기준 내림차순 정렬
                detections.sort((a, b) => b.confidence - a.confidence)
                return applyNMS(detections);
                // const bestDetection = detections[0];
                
                // // [x, y, width, height, classIndex] 형식으로 반환
                // return [
                //     bestDetection.box.x, 
                //     bestDetection.box.y, 
                //     bestDetection.box.width, 
                //     bestDetection.box.height,
                //     bestDetection.classIndex
                // ];
            }
            
            return null; // 감지된 객체가 없음
        };

        const initializeDetectResultBox = () => {
            detectResultBox.innerHTML = '';
        }

        fileInput.addEventListener('change', async (event) => {
            initializeDetectResultBox();
            const file = event.target.files[0];
            const reader = new FileReader();
            reader.onload = async (event) => {
                const tensor = await getImageTensorFromPath(event.target.result);
                const [imageInput, width, height] = await preProcessImage(event.target.result);
                const outputTensor = await runModel(imageInput, width, height);
                const boxes = await postProcessImage(outputTensor);
                
                if (!boxes) {
                    console.log('감지된 객체가 없습니다.');
                    return;
                }
                console.log('감지된 객체: ', boxes);

                if (!ctx) {
                    ctx = canvas.getContext('2d');
                }
                
                // 원본 이미지의 크기
                const canvasWidth = canvas.width;
                const canvasHeight = canvas.height;

                console.log('canvasWidth ', canvasWidth);
                console.log('canvasHeight ', canvasHeight);
                
                // 640x640에서 원본 이미지 크기로 좌표 변환
                const scale = Math.min(640 / canvasWidth, 640 / canvasHeight);
                const scaledWidth = canvasWidth * scale;
                const scaledHeight = canvasHeight * scale;
                const offsetX = (640 - scaledWidth) / 2;
                const offsetY = (640 - scaledHeight) / 2;
                
                for (let i = 0; i < boxes.length; i++) {
                    const box = boxes[i];
    
                    // YOLO 출력은 (중심 x, 중심 y, 너비, 높이)이므로 변환 필요
                    // 모델 출력 좌표는 640x640 스케일 기준
                    const centerX = box.box.x;
                    const centerY = box.box.y;
                    const width = box.box.width;
                    const height = box.box.height;
                    
                    // 원본 이미지 스케일로 변환
                    // 여기서는 입력 이미지가 패딩되어 640x640에 맞춰졌으므로 패딩 오프셋 고려
                    const scale = Math.min(640 / canvasWidth, 640 / canvasHeight);
                    const scaledWidth = canvasWidth * scale;
                    const scaledHeight = canvasHeight * scale;
                    const offsetX = (640 - scaledWidth) / 2;
                    const offsetY = (640 - scaledHeight) / 2;
                    
                    // 패딩을 고려한 좌표 계산 및 원본 이미지 스케일로 변환
                    const imgX = (centerX - offsetX) / scale;
                    const imgY = (centerY - offsetY) / scale;
                    const imgWidth = width / scale;
                    const imgHeight = height / scale;
                    
                    // 중심 좌표를 좌상단 좌표로 변환
                    const x = imgX - imgWidth / 2;
                    const y = imgY - imgHeight / 2;
                    
                    // 바운딩 박스 그리기
                    ctx.strokeStyle = "red";
                    ctx.lineWidth = 2;
                    ctx.font = "16px Arial";
                    ctx.fillStyle = "red";
                    ctx.beginPath();
                    console.log(box.className, x, y, imgWidth, imgHeight);
                    ctx.rect(x, y, imgWidth, imgHeight);
                    ctx.stroke();
                    
                    // 텍스트 위치 조정 - 바운딩 박스 상단에 표시
                    ctx.fillText(box.className + ' ' + box.confidence.toFixed(2), x, y - 5);

                    console.log(`Detect ${i+1}: ${cocoClassesDataSet[box.classIndex]}, Confidence: ${box.confidence.toFixed(2)}`);
                    const resultElement = document.createElement('span');
                    resultElement.textContent = `Detect ${i+1}: ${cocoClassesDataSet[box.classIndex]}, Confidence: ${box.confidence.toFixed(2)}`;
                    detectResultBox.appendChild(resultElement);
                }

                detectResult.textContent = `Detect Result: ${cocoClassesDataSet[boxes[0].classIndex]}`;
            }
            reader.readAsDataURL(file);
        });
    </script>
</body>
</html>